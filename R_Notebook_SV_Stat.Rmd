---
title: "Overview of Statistical Elements and Tests"
author: "Samuel Vincent"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
# Versions
* V1: Added general information on statistical elements and tests.
* V2: Created initial code blocks.
* V3: Revising code blocks for efficacy

# Preface
This R Notebook will contain information on various elements and tests used in statistical analyses. These will include t-tests, Wilcoxon tests, ANOVA, linear regressions and correlation tests, as well as elements such as hypotheses and p-values. In addition, I will be covering when it is appropriate to use each test and the assumptions of each one. Finally, I will be covering four types of data visualization methods: histograms, box plots, heat maps and  pie charts. I believe this will be a valuable asset for future statistical analyses on my end.

In addition to a general review of these elements, I will be covering blocks of R code that correspond to each element. This way, I can refer to this notebook in the likely case that I need to run a statistical test in R. These code blocks will come towards the end. 

# The Primary Statistical Tests
As mentioned before, the primary statistical tests are the t-test, Wilcoxon test, ANOVA, linear regression and correlation tests. These are the tools with which a data analyst will derive meaningful information from raw data.

## T-tests
### Overview
**T-tests** (or Student's T-tests) are used to determine the difference of **two means**, each from a different group. These are generally the first type of statistical test one will learn about, as it is the most frequently-used type. It only relies on a singular numerical variable, as opposed to multiple variables. 

It was initially created by **William S. Gosset**, an English statistician employed at the Guinness Brewery in Dublin. By the 1970s, this test was available in most software statistical packages.^[https://www.statisticshowto.com/probability-and-statistics/t-test/#assump]

### Assumptions
* When using a t-test, we have to assume:
  + The data follows a normal distribution
  + Observations of one group are independent of observations of the other
  + There are equal variances in the data
  
### When to Use It
T-tests should be used in scenarios where there are only two groups to derive means from. These can be two measurements of the same entity, or it can involve two entities that are being measured with unique properties or conditions.

### Hypotheses
* When using a t-test, the null and alternative hypotheses are:
  + Ho: There is not a difference between the two groups' means
  + Ha: There is a significant difference between the two groups' means
  
## Wilcoxon Tests
### Overview
The **Wilcoxon test** (a.k.a. Wilcoxon signed rank test) is similar to the t-test, with one significant difference: it is **nonparametric**. This means that the test doesn't assume that the data follows a distribution.^[https://statisticsbyjim.com/hypothesis-testing/wilcoxon-signed-rank-test/]

This test was created by **Frank Wilcoxon** in 1945, in the same scientific paper that also presented the rank-sum test. It was popularized by American psychologist and statistician Sidney Siegel in 1965.^[https://online.stat.psu.edu/stat415/lesson/20/20.2]
 
### Assumptions
* When performing a Wilcox test, we assume:
    + The data follows no particular distribution
    
### When to Use It
Statisticians will use Wilcox tests when the above assumption is true. In addition to this, this is preferred over t-tests because of increased power through its ability to analyze **ordinal data** (data that is categorized but differences between each category aren't identifiable) and reduce outlier impact.

### Hypotheses
Since there are two types of Wilcox tests, one-sample and paired, there are a pair of null and alternative hypotheses for each one.

* For the one-sample test:
   + Ho: The population median is the same as the benchmark value
   + Ha: The population median is significantly different from the benchmark value
* For the paired test:
    + Ho: The median of the paired differences cancel each other out (=0) in the overall population
    + Ha: The median of the paired differences don't equal zero in the overall population

## ANOVA
### Overview
The **ANOVA** (a.k.a. Analysis of Variance) test is used to see of the means of **three or more groups** for significant differences among them.^[https://researchmethod.net/anova/] This assists statisticians with determining whether differences observed in the groups' means are due to chance or caused by true effects of the property applied to them. This makes it similar to the t-test, only it has the capacity for 3+ groups' means.

This test was devised by **R. A. Fisher**, a British statistician, in 1918. It was developed using the t-test as a reference point, which explains its similarities.^[https://www.britannica.com/topic/variance-analysis-statistics]

### Assumptions
* When utilizing an ANOVA, we assume:
    + The data follows a normal distribution
    + Observations of one group are independent of observations of the other
    + There are equal variances in the data

Note that these assumptions are the same as with a t-test. 

### When to Use It
People typically use ANOVA tests when there are three or more groups of data from which means can be derived. By grouping all of these data together, this consequently reduces the risk of false significance in the results.

### Hypotheses
* The null and alternative hypotheses of an ANOVA test are almost the same as with a t-test, which are:
    + Ho: There is no difference between the means of the 3+ groups of data
    + Ha: There is a significant difference between the means of the 3+ groups of data. More specifically, at least one group of data contains a significantly-different mean from the others
    
## Linear Regression
### Overview
**Linear regression** is a statistical model that details the relationships between at least one **explanatory variable** (any variable that predicts changes in another variable^[https://statisticsbyjim.com/glossary/explanatory-variable/]) and an **outcome variable** (the variable that's being affected by the explanatory variable^[https://statisticsbyjim.com/glossary/response-variable/]). ^[https://statisticsbyjim.com/regression/linear-regression/#google_vignette]

It is one of the earliest types of regression analyses.

### Assumptions
* When conducting a linear regression analysis, we assume:
    + Normal distribution of data
    + A linear formula fits the variables' relationship
    + Constant scattering among residuals
    + Any observations of the data are independent from each other
    
### When to Use It
Linear regression analyses are used when the relationship between variables are desired, as well as when this relationship is thought to be represented by a linear formula, such as y=mx+b. This in turn serves to predict changes in certain variables by changes in others, the basis of clinical research. 

### Hypotheses
* The null and alternative hypotheses of the linear regression analysis are:
    + Ho: The slope of the population equals zero (in the formula y=β₀+β₁+ε
 ). The hypotheses focus on the status of β₁ in this instance^[https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Mostly_Harmless_Statistics_(Webb)/12%3A_Correlation_and_Regression/12.02%3A_Simple_Linear_Regression/12.2.01%3A_Hypothesis_Test_for_Linear_Regression]
    + Ha: The slope of the population doesn't equal zero
    
## Correlation Test
### Overview
The **correlation test** (a.k.a. correlation hypothesis test) is used to determine the strength and direction of the relationship between two variables.^[https://stats.libretexts.org/Courses/Rio_Hondo_College/Math_130%3A_Statistics/11%3A_Correlation/11.03%3A_Correlation_Hypothesis_Test] This statistical test does this through use of the correlation coefficient and sample size. 

### Assumptions
* When using this test, we assume:
    + Normal data distribution
    + A linear relationship between two variables of interest^[https://usq.pressbooks.pub/statisticsforresearchstudents/chapter/correlation-assumptions/]
    + Minimal outliers

### When to Use It
We use the correlation hypothesis test when we want to determine whether the relationship between two non-outcome variables is significant or not.^[https://www.statology.org/when-to-use-correlation/] This makes it similarly useful in clinical trials, compared to linear regression analyses.
    
### Hypotheses
* The null and alternative hypotheses of this test are:
    + Ho: There isn't a significant linear relationship between the two variables in question
    + Ha: The relationship between these two variables is both linear and significant. There is an identifiable correlation between them
    
# The Primary Statistical Elements
Alongside statistical tests, there are two important elements present in virtually all quantitative analyses: the **hypotheses** and **p-value**. The former are developed towards the start of the experiment/study, and the latter is derived towards the end as part of the results. Without them, no one would be able to put forth meaningful information through such scientific endeavors. 

To this end, I will provide information on these elements. I hope to keep this so I can look back on it in the future.

## The Hypotheses
The hypotheses are composed of both the null and alternative hypotheses. These are devised in order to determine the potential outcomes of a given study/experiment/trial/etc. It's similar to a traditional scientific hypothesis, with the difference being that these outcomes are listed in the most statistically-technical manner possible, to prevent assumptions on the potential results. I've already described various hypotheses in the previous section, but I will go over each type in greater detail here.

### Null Hypothesis
The **null hypothesis** (commonly referred to as "h~o~" or "ho") will always state that there are no differences between sets of data, whether this is before and after a treatment, when comparing statistical values of different sets of data, etc. Even if differences are found, the h~o~ will write this off as a result of random error/variation in the data sets.

### Alternative Hypothesis
The **alternative hypothesis** (referred to as "h~a~" or "ha") will state the opposite of the former's proposition. This means that any differences found in data sets after performing the necessary tests are a result of direct, intentional intervention. In other words, if a clinical trial was being conducted, this hypothesis will state that the treatment being applied to the experimental group is directly affecting the results. 

### Making a Decision
When determining the final results of your experiment/study, you'll need to either **reject the h~o~ or accept it**. The former signifies that you've determined that a difference was made because of the applied treatment, while the latter means that you've determined that there was no actual difference in the populations.^[https://courses.lumenlearning.com/introstats1/chapter/null-and-alternative-hypotheses/]

## P-Values
While the null and alternative hypotheses are integral to setting up an experiment, **p-values** (a.k.a. probability values) are equally as such when closing one out. A p-value is the measure of confidence, or likelihood, of obtaining the same set of data when performing the experiment/study under the assumption of the h~o~.^[https://www.investopedia.com/terms/p/p-value.asp] In other words, a smaller p-value points more strongly to the h~a~. This makes obtaining as small of a p-value as possible desirable among researchers. 

While people generally try getting their p-values as small as possible (unless you've set out to disprove previous studies), a rule of thumb is that a p-value of 0.05 or less is considered sufficiently likely that the h~a~ is the true conclusion. This condition is known as **statistical significance**; If your experiment yields a p-value ≤ 0.05, it is considered statistically-significant. This is very good news if you're conducting a clinical trial! In this case, it would mean your treatment has a noticeable effect in the subjects, and can't reasonably be written off as random data variation.

# Visualizing Data
As demonstrated, there is a multitude of ways data can be manipulated to reach a desired result. However, it's almost just as important to be able to show yourself and others these data in an easier-to-digest form. No one wants to shovel through a sea of numbers to see what you're trying to prove. This is where **data visualization** comes in. Data visualization enables your peers to see the results of your efforts more easily. 

There are four primary methods of displaying such data, with each one depending on the nature of the data: **histograms, box plots, heat maps and pie charts**. I will be covering each one in the upcoming material.

## Histograms
### Overview
Histograms consist mainly of x and y-axes, combined with boxes of various lengths lined up horizontally; Legends and trend-lines usually compliment this as well. These are used to display how data is distributed.^[https://www.geeksforgeeks.org/maths/histogram/]. Due to the nature of histograms, they're commonly used to help visualize trends in a set of data across a continuous interval, or period. 

The difference between histograms and **bar charts** is a matter of the properties of the displayed data. While bar charts show categorical data with no need to display the period over which the data exists, histograms display how one set of data exists over a continuous interval, as stated before. The chunk of period a statistician will cut out for a histogram is called a **bin**.

Here is a link to an image of a typical histogram:
https://www.geeksforgeeks.org/maths/histogram/

### Terms to Know
* Statistical entities associated with histograms include:
    + **Central tendency**: Which value the majority of recorded samples fall into. This is represented as a peak in the histogram. There can be more than one peak, depending on data.^[https://statisticsbyjim.com/basics/histograms/]
    + **Symmetry**: The bars on a histogram chart can either be of the same frequency on both sides of the peak, or it can tend towards one side.
    + **Variability**: A measure of how much of a peak the chart displays. More variability yields a less-defined, more rounded-out peak. 
    
## Box Plots
### Overview
Box plots are very similar to histograms, except that multiple intervals can be displayed in one graph. This makes it useful for researchers, as it can display a lot of information in a single image.^[https://statisticsbyjim.com/graphs/box-plot/]. It's basically a simplified version of the data presented in multiple histograms. Box plots don't make assumptions on the distribution of the data, making it nonparametric. 

Here is a link to an image of a typical box plot: https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2019/01/boxplot_teaching.png?w=576&ssl=1

### Terms to Know
* Box plots outline their central tendencies and spreads through the **5-number summary**, which consist of:
    + **Minimum**: The smallest number in the interval's data set.
    + **First quartile**: The value that separates the bottom 25% of values from the rest of the data.
    + **Median**: The middle value of the data set. Can also be called the **second quartile**.
    + **Third quartile**: The value that separates the bottom 75% of values from the rest of the data.
    + **Maximum**: The largest number in the data set.
    
## Heat Maps
### Overview
Heat maps are typically used with large sets of data, all with values that can differ to various intensities.^[https://www.geeksforgeeks.org/data-visualization/what-is-heatmap-data-visualization-and-how-to-use-it/] This data visualization technique will chart the data in a color-coded fashion, according to a spectrum of values. Heat maps are typically displayed in a **2d or 3d** matrix, and use a range of values as a reference point for a color spectrum. 

For example, a heat map can graph the temperature distribution of a given area, with a spectrum ranging from red to blue; Redder colors are higher temperatures, while bluer colors are lower. The axes can represent the physical area the temperatures are being recorded over.

Here is an example of a typical 2-dimensional heat map... https://www.researchgate.net/publication/344202320/figure/fig5/AS:959980165414946@1605888515416/A-heat-map-of-the-two-dimensional-hierarchical-cluster-analysis-suggesting-differential.png

...As well as an example of a 3-dimensional one: https://drjohnsullivan.com/wp-content/uploads/2016/04/heat-map.jpg

### Terms to Know
* Some terms that are useful to know about when dealing with heat maps are:
    + **Matrix**: The area the axes give rise to. This can be either a 2 or 3-dimensional space where the values are plotted out. 
    + **Spectrum**: The range of values being depicted in the heat map's matrix, being displayed as a literal color spectrum. 
    + **Clustering**: The property of a heat map that helps group similar rows and columns together. Depicted as tree-like structures on the side of the matrix.
    
## Pie Charts
### Overview
Pie charts are likely one of the first data visualization methods one will use. This can be chalked up to their ease of use; However, they tend to be one of the most misused. ^[https://www.storytellingwithdata.com/blog/2020/5/14/what-is-a-pie-chart] A pie chart consists of a circle, with categorical data being represented on it as "slices." Larger slices mean it takes up a larger portion of a single population/variable.

These charts can come in a variety of shapes, such as rings and bars.

Here is an example of one: https://images.squarespace-cdn.com/content/v1/55b6a6dce4b089e11621d3ed/1589459527969-AMZ4OHKJ5ES2GLYKK9WV/2_sales+by+product.png?format=1000w

### Terms to Know
* Some terms to know when dealing with pie charts are:
    + **Slice**: The portion of a whole variable a singular piece of categorical data takes up, visually represented. Its size can also depict frequency something happens in one instance. 
    + **Pie**: The whole variable/population/instance. A typical pie represents **one** of these. Another type of chart would be needed if you were plotting categorical data across an interval. 
    
# R Code Blocks: Statistical Tests
Now that we've covered all of the general information, we can integrate this knowledge into our R code. These chunks will be displayed in distinguishable code blocks. Keep in mind that the first published version of this notebook doesn't have this section. 

## T-Tests
There are three types of T-Tests you can perform in R: One-sample, two-sample and paired. For the one-sample t-test, we include a known value (in this case, 3.5), making this code expressed as:
```{r}
data <- c(1,2,3,4,5)
t.test(data, mu = 3.5)
```

For a two-sample t-test, we could write something like this. In this instance, we assume the variances between the two groups to be equal, indicated by the argument "var.equal = TRUE". We use this type of t-test to compare two **independent** groups:
```{r}
group_one <- c(1,3,5,7,9)
group_two <- c('A','B','C','D','E')
group_three <- c(0,2,4,6,8)
t.test(group_one, group_three, var.equal = TRUE)
```

And finally, for a paired t-test, we can write something along the lines of this. Unlike the previous type, this is used to compare two **related** groups:
```{r}
group_before <- c(1,3,5,7,9)
group_after <- c(7,23,78,12,7)
t.test(group_before, group_after, paired = TRUE)
```

## Wilcoxon Tests
Similarly to the t-tests, Wilcoxon tests come in three flavors for the sake of R: Signed-rank tests, rank-sum tests and one-sample tests. Signed-rank tests are used typically to compare data of two **related** samples. This could be expressed as:
```{r}
group_before <- c(1,3,5,7,9)
group_after <- c(0,2,4,6,8)
wilcox.test(group_before, group_after, paired = TRUE)
```

Rank-sum tests are used to compare two **independent** samples of data. In this case, such a test could be written like:
```{r}
group_one <- c(1,3,5,7,9)
group_two <- c(4,8,45,21,92,12)
wilcox.test(group_one, group_two, paired = FALSE)
# Keep in mind that the only difference between the sign-rank and rank-sum tests are whether you mark the "paired" argument as TRUE or FALSE.
```

Lastly, we use one-sample test to determine whether the median of one sample deviates from a known value (in this case, 3.2):
```{r}
sample <- c(1,2,3,4,5,11,24,31)
wilcox.test(sample, mu = 3.2)
```

## ANOVA Tests
There are two types of ANOVA tests we can perform in R: one-way and two-way. We use the former whenever we have one **categorical, independent** variable and one **continuous, dependent** variable:
```{r}
# rnorm stays the same, while the means from each group can differ.
data <- data.frame(
  group = rep(c('A', 'B', 'C'), each = 10),
  value = c(rnorm(10, mean = 2), rnorm(10, mean = 5), rnorm(10, mean = 8))
)

anova_result <- aov(value ~ group, data = data)
anova_result
```

For the two-way ANOVA, this is used whenever there are **two** categorical, independent variables. This can be written out as:
```{r}
data <- data.frame(
  group1 = rep(c("A", "B"), each = 5),
  group2 = rep(c("Y", "Z"), times = 5),
  value = c(rnorm(5, mean = 5), rnorm(5, mean = 4.5), rnorm(5, mean = 3), rnorm(5, mean = 2.5))
)

anova_result_2way <- aov(value ~ group1 * group2, data = data)
anova_result_2way
# The only difference between the two types is whether you set your value up with only one group, or two. 
```

## Linear Regressions
Compared with the last three tests, linear regressions are simple to perform in R. We will use a base data set, "mtcars" for the purpose of this exercise. Keep in mind that there are multiple steps, however; We can write an instance of this out as:
```{r}
# Load in and inspect your data and related packages
library(ggplot2)
data(mtcars)
head(mtcars)
summary(mtcars)

# Perform the linear regression
# Format: model <- lm(dep_variable ~ indep_variable, data = df)
model <- lm(mpg ~ wt, data = mtcars)
```
Note how, upon executing this code, the console gives us a 5-number summary for each column in the mtcars data frame, as well as the linear regression's visual expression.

Next, we can visualize this model with the following code:
```{r}
plot(mtcars$wt, mtcars$mpg)
abline(model, col = "blue")
# You can use any color you like! It just needs to be in-between quotation marks, following the "col" argument.

# If you're performing a multiple regression test, you can try a diagnostic plot, which looks like...
par(mfrow = c(2, 2))
plot(model)
```

Finally, we can use this test to predict new values with the help of the predict() function. We can achieve this through writing the following code. As you can see by the result, the predicted miles per gallon would be 10.56.
```{r}
new_data <- data.frame(wt = 5.0)  # We're using an example weight of 5k pounds, hence the 5.0
mpg_predictions <- predict(model, newdata = new_data)
mpg_predictions
```

## Correlation Tests
There are three methods we can use when performing correlation tests in R: 'pearson', 'spearman' and 'kendall'. We use the pearson method when we're dealing with **linear relationships** between target variables, the spearman method when we're dealing with **monotonic^[Both variables have the same direction, but rates of change may differ.]** relationships, and the kendall method for **ordinal data^[Any data that has a natural, ordered sequence.]** . We can write these out as:
```{r}
# The pearson method
x <- c(1,2,3,4,5)
y <- c(0,2,4,6,8)
p_result <- cor.test(x, y, method = "pearson")

# The spearman method
x <- c(1,2,3,4,5)
y <- c(0,2,4,6,8)
s_result <- cor.test(x, y, method = "spearman")

# The kendall method
x <- c(1,2,3,4,5)
y <- c(0,2,4,6,8)
k_result <- cor.test(x, y, method = "kendall")

# Displaying a given result
p_result
```

# R Code Blocks: Hypotheses & P-Values
We can set up logical systems to facilitate the integration of these elements in R. We can try out a way to do this with both of them in one code block, since they can only exist in the context of each other. Now, let's try this setup with some data from the mtcars data frame. We can use data from the mpg column for this:
```{r}
# Loading in and inspecting the data 
update.packages(ask = FALSE)
install.packages("plotrix")
library(plotrix)
data <- data(mtcars)
summary(mtcars)

# Now we know that the population mean of the cars' mpgs equals 20.09; However, we also need a sample mean for this model to work. To this end, let's take a sample of the mpg values belonging to the first 3 cars in the list. 
population_mean_cars <- 20.09
sample_mean_cars <- ((21.0 + 21.0 + 22.8)/3)

# We also need standard error, which needs a standard deviation value. We can calculate these values by the following code and by using the neat plotrix package installed at the start:
mpg_column <- mtcars$mpg
mpg_column

se_mpg <- std.error(mpg_column)

# Finally, we can determine the z-score and p-value:
z_score_mpg <- (sample_mean_cars - population_mean_cars) / se_mpg
p_value_mpg <- 2 * pnorm(-abs(z_score))

# Setting up the logical system:
if(p_value <= 0.05) {
  result_mpg <- print("The results indicate rejection of the Null Hypothesis!")
} else if(p_value > 0.05) {
  result_mpg <- print("The results indicate acceptance of the Null Hypothesis!")
}
result_mpg
```

Keep in mind that this is only one of many ways you can setup a logical system to help you make your final decision. This was just one of the more basic ways. You can plug in just about any test you want in the first portion of the code block, and go from there.

# R Code Blocks: Visualizing Data
Lastly, we will cover how to create visual plots in R. Refer to the general information section regarding these plots if you need help understanding certain related concepts within the code blocks.

## Histograms
We can code for a histogram by using the **hist()** function and inserting placeholder data: 
```{r}
# Format: hist(df, main = histogram_title, xlab = "x_axis_label", col = "column_color", border = "border_color")
data <- c(1,2,3,4,5,10,20,30,50,100)

hist(data, main = "Hystogram", xlab = "Values", col = "lightgrey", border = "black")
```

## Box Plots
We can code for a box plot with the **boxplot()** function and writing something along these lines:
```{r}
# Format: boxplot(df, main = "boxplot_title", ylab = "y_axis_label", col = "column_color")
data <- c(1,2,3,4,5,10,20,30,50,100)

boxplot(data, main = "Basic Box Plot", ylab = "Values", col = "lightgreen")
```

For multiple groups of data, we can do something like this:
```{r}
group1 <- c(1,3,5,7,9)
group2 <- c(0,2,4,6,8)

data_list <- list(Group1 = group1, Group2 = group2)

boxplot(data_list, main = "Multi-Group Box Plot", ylab = "Values", col = c("lightblue", "pink"))
```

## Heat Maps
Heat maps can be coded for in R using the **heatmap()** function, and by writing the following:
```{r}
# Format: heatmap(df, main = "heatmap_title", col = heat.colors(n))
data <- matrix(rnorm(100), nrow = 10)

heatmap(data, main = "Base R Heatmap", col = heat.colors(10))
```

The **ggplot2** package, if installed, can be used to create plots with more customization options, including heat maps, with the use of the **ggplot()** function. 

These are just placeholder values; You can plug any number/color you need to properly display your data.
```{r}
library(ggplot2)

data <- expand.grid(X = 1:10, Y = 1:10)
data$Value <- rnorm(100)

ggplot(data, aes(x = X, y = Y, fill = Value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightgrey", high = "blue") +
  labs(title = "The Better Heatmap") +
  theme_minimal()
```

## Pie Charts
Unsurprisingly, there is a dedicated function in Base R for the creation of pie charts: **pie()**. Here's how you could create one:
```{r}
# Format: pie(slices_list, labels = labels_list, main = "pie_chart_title", col = "column_color")
# column_color can be a gradient, based on the slices' length. This is indicated by the modified argument "gradient_color(length(slices))"
slices <- c(1,5,10,30,15,7.5)
labels <- c("Category A", "Category B", "Category C", "Category D")

pie(slices, labels = labels, main = "Life of Pie", col = rainbow(length(slices)))
```

# Conclusion
And there you have it! There are a variety of ways in which a statistician can gather and process data to prove a point. Each test has a rich history of use, with their creators working tirelessly to finalize their methods. There are equally as many ways to visualize this data as there are methods to process it as well, each with unique conditions for use. I hope this notebook was illuminating for you; I will also be keeping this handy for future reference. 